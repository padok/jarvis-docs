{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"JARVIS - Markerless 3D Motion Capture Toolbox JARVIS makes highly precise markerless 3D motion capture easy. All you need to get started is a multi camera recording setup and an idea of what you want to track. Our Toolbox will assist you on every step along the way, from recording synchronised videos, to quickly and consistently annotating your data, all the way to the final 3D pose predictions. If you are interested in setting up a 3D Motion Capture System or just want to learn more about our toolbox we strongly recommend having a look at our Getting Started Guide and our Manual . Here you'll find an overview of our workflow as well as tutorials to help you build a successful 3D tracking pipeline with JARVIS. Why JARVIS? Markerlerss motion capture has become an essential data analysis tool in many fields of research, pioneered largely by DeepLabCut several years ago. While there have been significant improvements to those methods both in terms of capabilities and usability, those advances mainly tackle single image based 2D tracking. Setting up a system for multi camera 3D motion capture on the other hand still remains very challenging and time consuming.\\ JARVIS tries to change that by introducing an easy to use toolchain that was developed with highly precise 3D Tracking as the primary goal, not just an afterthought. Our System consists of three parts: The Acquisition Tool allows you to record synchronized high resolution videos from multiple cameras at high frame rates thanks to GPU accelerated online compression. If you use one of the recommended cameras you can get your system setup and running with just a handful of clicks. The Annotation Tool leverages the multi camera recordings by projecting your manual annotations on a subset of those cameras to the remaining ones, significantly reducing the amount of tedious manual annotations needed. It also provides easy-to-use interfaces for calibrating your cameras and creating datasets. The HybridNet Pytorch Library is our state of the art 3D pose estimation architecture. The hybrid 2D/3D CNN architecture is faster to train and more precise than pure 3D CNNs and most importantly it can run on siginficantly smaller dataset sizes, saving you countless hours of mind numbing annotations while still achieving highly precise markerless motion capture - even in scenarios with heavy occlusion. Human Hand Annotated Frames 10000 Number of subjects 4 Monkey Hand Annotated Frames 3000 Number of subjects 1 Rat Full Body Annotated Frames 2000 Number of subjects 1 Mouse Full Body Annotated Frames 3000 Number of subjects 3 Our model outperforms the other currently available methods when trained on our human hand tracking dataset. Below you can see the performance of our toolbox compared to the well established markerless tracking toolbox DeepLabCut (DLC) and the recently released Dannce 3D tracking toolbox: Supported Cameras As mentioned before to be able to take full advantage of our Acquisition Tool without any modifications you'll have to use one of our supported cameras. The table below shows all currently supported camera models, we'll try to expand this list in the near future! Company Model Name FLIR Chameleon FLIR Blackfly S FLIR Grasshopper","title":"Home"},{"location":"#jarvis-markerless-3d-motion-capture-toolbox","text":"JARVIS makes highly precise markerless 3D motion capture easy. All you need to get started is a multi camera recording setup and an idea of what you want to track. Our Toolbox will assist you on every step along the way, from recording synchronised videos, to quickly and consistently annotating your data, all the way to the final 3D pose predictions. If you are interested in setting up a 3D Motion Capture System or just want to learn more about our toolbox we strongly recommend having a look at our Getting Started Guide and our Manual . Here you'll find an overview of our workflow as well as tutorials to help you build a successful 3D tracking pipeline with JARVIS.","title":"JARVIS - Markerless 3D Motion Capture Toolbox"},{"location":"#why-jarvis","text":"Markerlerss motion capture has become an essential data analysis tool in many fields of research, pioneered largely by DeepLabCut several years ago. While there have been significant improvements to those methods both in terms of capabilities and usability, those advances mainly tackle single image based 2D tracking. Setting up a system for multi camera 3D motion capture on the other hand still remains very challenging and time consuming.\\ JARVIS tries to change that by introducing an easy to use toolchain that was developed with highly precise 3D Tracking as the primary goal, not just an afterthought. Our System consists of three parts: The Acquisition Tool allows you to record synchronized high resolution videos from multiple cameras at high frame rates thanks to GPU accelerated online compression. If you use one of the recommended cameras you can get your system setup and running with just a handful of clicks. The Annotation Tool leverages the multi camera recordings by projecting your manual annotations on a subset of those cameras to the remaining ones, significantly reducing the amount of tedious manual annotations needed. It also provides easy-to-use interfaces for calibrating your cameras and creating datasets. The HybridNet Pytorch Library is our state of the art 3D pose estimation architecture. The hybrid 2D/3D CNN architecture is faster to train and more precise than pure 3D CNNs and most importantly it can run on siginficantly smaller dataset sizes, saving you countless hours of mind numbing annotations while still achieving highly precise markerless motion capture - even in scenarios with heavy occlusion. Human Hand Annotated Frames 10000 Number of subjects 4 Monkey Hand Annotated Frames 3000 Number of subjects 1 Rat Full Body Annotated Frames 2000 Number of subjects 1 Mouse Full Body Annotated Frames 3000 Number of subjects 3 Our model outperforms the other currently available methods when trained on our human hand tracking dataset. Below you can see the performance of our toolbox compared to the well established markerless tracking toolbox DeepLabCut (DLC) and the recently released Dannce 3D tracking toolbox:","title":"Why JARVIS?"},{"location":"#supported-cameras","text":"As mentioned before to be able to take full advantage of our Acquisition Tool without any modifications you'll have to use one of our supported cameras. The table below shows all currently supported camera models, we'll try to expand this list in the near future! Company Model Name FLIR Chameleon FLIR Blackfly S FLIR Grasshopper","title":"Supported Cameras"},{"location":"developer_documentation/1_introduction/","text":"Developer Documentation TODO","title":"1. Introduction"},{"location":"developer_documentation/1_introduction/#developer-documentation","text":"TODO","title":"Developer Documentation"},{"location":"developer_documentation/9_trigger/","text":"TriggerFirmware Development using PlatformIO PlatformIO PlatformIO is an software ecosystem that greatly simplifies the programming of microcontrollers. It is available as a plugin for many programming environments and can also be run directly from the command line. Installing PlatformIO We recommend using one of the options below to flash your microcontroller: Visual Studio Code (VSCode) Installation Guide: See: https://platformio.org/platformio-ide After PlatformIO is successfully installed, clone the trigger firmware from Github using git: Use \"Clone Git Repository\" in VS Code and type: https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git in the window that opens. After that, open the folder manually or press open folder when VS Code asks you for the Git repository you just cloned: . If PlatformIO is installed correctly, all the required dependencies should be installed automatically. After the project is set up correctly, open the PlatformIO tab on the left side of the page If your microcontroller is officially supported, connect your microcontroller to your machine, open the context menu of the microcontroller you want to flash and press \"Upload\". CLI Installation Guide: See: https://platformio.org/install/cli Make sure you can run the commands in your terminal: platformio run --help Flashing a microcontroller If your microcontroller is supported, you can find a corresponding environment in JARVIS-TriggerFirmware/platformio.ini . To flash your microcontroller, execute: platformio run -t upload -e <YOUR_MCU_ENVIRONMENT> Wireing Guide The default behavior for all microcontrollers with enough pins is that pins 0-15 are available as trigger outputs for your cameras and pins 16-23 are pullup enabled input pins for the timed logging feature of the Acquisition software. If your microcontroller does not have the required number of pins, or does not have support for functionality such as digital interrupt on all input pins, you should look at the respective header files in the boards/ folder. Trigger protocol The protocol is build for two use cases in mind. The first one is to to give the microcontroller commands, mainly the command to start and stop the triggers that we use for Acquisition. Like shown below: Recording PC Recording PC MCU MCU setup (Frames) ack The ability of the microcontroller to also read inputs and notify the acquisition software is the second use case. For this purpose, the microcontroller sends a message at every COM-loop, if a change has occurred. Recording PC Recording PC MCU MCU Inputs ack Warning ACKs are not yet handled by the MCU. Retransmissions are still being investigated. Message encoding Bytes COBS encoded Packet delimiter Packet Structure header payload 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Input Message TYPE_INPUTS Packet Structure header payload Bytes type length crc inputs uptime_us pulse_id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Setup Message TYPE_SETUP Packet Structure header payload Bytes type length crc pulse_hz pulse_limit delay_us flags 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Bit Flag Description 0 RESET_COUNTER If set to true (1) the setup command will reset the current Framecounter to 0 1-7 RESERVED Reserved flag for later functionality Type-Only Messages TYPE_ACK Packet Structure header Bytes type length crc 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Char Messages TYPE_ECHO TYPE_TXT TYPE_ERROR Packet Structure header payload Bytes type length crc char[] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14","title":"9. Trigger"},{"location":"developer_documentation/9_trigger/#triggerfirmware","text":"","title":"TriggerFirmware"},{"location":"developer_documentation/9_trigger/#development-using-platformio","text":"","title":"Development using PlatformIO"},{"location":"developer_documentation/9_trigger/#platformio","text":"PlatformIO is an software ecosystem that greatly simplifies the programming of microcontrollers. It is available as a plugin for many programming environments and can also be run directly from the command line.","title":"PlatformIO"},{"location":"developer_documentation/9_trigger/#installing-platformio","text":"We recommend using one of the options below to flash your microcontroller:","title":"Installing PlatformIO"},{"location":"developer_documentation/9_trigger/#visual-studio-code-vscode","text":"Installation Guide: See: https://platformio.org/platformio-ide After PlatformIO is successfully installed, clone the trigger firmware from Github using git: Use \"Clone Git Repository\" in VS Code and type: https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git in the window that opens. After that, open the folder manually or press open folder when VS Code asks you for the Git repository you just cloned: . If PlatformIO is installed correctly, all the required dependencies should be installed automatically. After the project is set up correctly, open the PlatformIO tab on the left side of the page If your microcontroller is officially supported, connect your microcontroller to your machine, open the context menu of the microcontroller you want to flash and press \"Upload\".","title":"Visual Studio Code (VSCode)"},{"location":"developer_documentation/9_trigger/#cli","text":"Installation Guide: See: https://platformio.org/install/cli Make sure you can run the commands in your terminal: platformio run --help","title":"CLI"},{"location":"developer_documentation/9_trigger/#flashing-a-microcontroller","text":"If your microcontroller is supported, you can find a corresponding environment in JARVIS-TriggerFirmware/platformio.ini . To flash your microcontroller, execute: platformio run -t upload -e <YOUR_MCU_ENVIRONMENT>","title":"Flashing a microcontroller"},{"location":"developer_documentation/9_trigger/#wireing-guide","text":"The default behavior for all microcontrollers with enough pins is that pins 0-15 are available as trigger outputs for your cameras and pins 16-23 are pullup enabled input pins for the timed logging feature of the Acquisition software. If your microcontroller does not have the required number of pins, or does not have support for functionality such as digital interrupt on all input pins, you should look at the respective header files in the boards/ folder.","title":"Wireing Guide"},{"location":"developer_documentation/9_trigger/#trigger-protocol","text":"The protocol is build for two use cases in mind. The first one is to to give the microcontroller commands, mainly the command to start and stop the triggers that we use for Acquisition. Like shown below: Recording PC Recording PC MCU MCU setup (Frames) ack The ability of the microcontroller to also read inputs and notify the acquisition software is the second use case. For this purpose, the microcontroller sends a message at every COM-loop, if a change has occurred. Recording PC Recording PC MCU MCU Inputs ack Warning ACKs are not yet handled by the MCU. Retransmissions are still being investigated.","title":"Trigger protocol"},{"location":"developer_documentation/9_trigger/#message-encoding","text":"Bytes COBS encoded Packet delimiter Packet Structure header payload 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14","title":"Message encoding"},{"location":"developer_documentation/9_trigger/#input-message","text":"TYPE_INPUTS Packet Structure header payload Bytes type length crc inputs uptime_us pulse_id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14","title":"Input Message"},{"location":"developer_documentation/9_trigger/#setup-message","text":"TYPE_SETUP Packet Structure header payload Bytes type length crc pulse_hz pulse_limit delay_us flags 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Bit Flag Description 0 RESET_COUNTER If set to true (1) the setup command will reset the current Framecounter to 0 1-7 RESERVED Reserved flag for later functionality","title":"Setup Message"},{"location":"developer_documentation/9_trigger/#type-only-messages","text":"TYPE_ACK Packet Structure header Bytes type length crc 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14","title":"Type-Only Messages"},{"location":"developer_documentation/9_trigger/#char-messages","text":"TYPE_ECHO TYPE_TXT TYPE_ERROR Packet Structure header payload Bytes type length crc char[] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14","title":"Char Messages"},{"location":"downloads/downloads/","text":"Downloads Here you can find the installers for the AnnotationTool and the AcquisitonTool for all currently supported operating systems.\\ Please checkout our Github Repositories if you want to build our tools from source. AnnotationTool Windows 64 bit MacOS (Catalina or newer) Ubuntu 22.04 Ubuntu 20.04 Ubuntu 18.04 AcquisitionTool Windows 64 bit Ubuntu 20.04 Ubuntu 18.04 Trigger Firmware","title":"Downloads"},{"location":"downloads/downloads/#downloads","text":"Here you can find the installers for the AnnotationTool and the AcquisitonTool for all currently supported operating systems.\\ Please checkout our Github Repositories if you want to build our tools from source. AnnotationTool Windows 64 bit MacOS (Catalina or newer) Ubuntu 22.04 Ubuntu 20.04 Ubuntu 18.04 AcquisitionTool Windows 64 bit Ubuntu 20.04 Ubuntu 18.04 Trigger Firmware","title":"Downloads"},{"location":"getting_started/1_introduction/","text":"Getting Started with JARVIS Welcome to our Getting Started Guide! This guide will teach you how to set up the three different parts of our toolbox and how to work with our example data. This will help you get an idea of all the steps required to build and use a 3D markerless motion capture setup. After working through this guide you will be ready to have a look at our Manual and learn all the things necessary to build you own motion capture system from scratch. Note that this Guide is somewhat back to front. You will learn how to work with annotated data before we show you how to actually annotate it. One additional note, JARVIS uses two different formats for the annotated data. The one you will work with first is called the trainingset . A trainingset is a finished set of annotations in the right format to be used by the JARVIS Pytorch module. The second one is called a dataset , this is the format that is used by the AnnotationTool. We know this can be a bit confusing, but the two different formats are neccessary to make some features of JARVIS work.","title":"1. Introduction"},{"location":"getting_started/1_introduction/#getting-started-with-jarvis","text":"Welcome to our Getting Started Guide! This guide will teach you how to set up the three different parts of our toolbox and how to work with our example data. This will help you get an idea of all the steps required to build and use a 3D markerless motion capture setup. After working through this guide you will be ready to have a look at our Manual and learn all the things necessary to build you own motion capture system from scratch. Note that this Guide is somewhat back to front. You will learn how to work with annotated data before we show you how to actually annotate it. One additional note, JARVIS uses two different formats for the annotated data. The one you will work with first is called the trainingset . A trainingset is a finished set of annotations in the right format to be used by the JARVIS Pytorch module. The second one is called a dataset , this is the format that is used by the AnnotationTool. We know this can be a bit confusing, but the two different formats are neccessary to make some features of JARVIS work.","title":"Getting Started with JARVIS"},{"location":"getting_started/2_exploring_example/","text":"Exploring the Provided Example Trainingset Let's start by playing around with our provided example so you can familiarize with our software and get a better feel for the task and the workflow. The example data we're working with in this tutorial are recordings of one of our monkeys performing a simple grasping task in our 12 camera setup. Your task is to track his hand while he is enjoying a variety of fruits we hand him. We will split the task into four steps: Installing our Pytorch Toolbox and downloading the example recordings. Visualizing the provided annotations, both in 2D and 3D. Training the entire network stack. Predicting Poses for the Example Recording. Creating Annotated Videos from Your Predictions. 1. Installing the Toolbox and Downloading the Data First let's take care of setting up the software. Make sure you have a version of Anaconda installed. If you want to train networks also make sure that your PC has a Nvidia GPU with working CUDA drivers installed. There are only a few simple steps you need to take to install the toolbox: - Download the python package. To do this open up a terminal and run: git clone https://github.com/JARVIS-MoCap/JARVIS-HybridNet.git && cd JARVIS-HybridNet Alternatively you download it directly by clicking here . Create the jarvis Anaconda environment by running: conda create -n jarvis python=3.9 pytorch=1.10.1 torchvision cudatoolkit=11.3 notebook -c pytorch Activate the environment (you will need to do this every time you open a terminal to use JARVIS): conda activate jarvis Install the required version of the setuptools package: pip install -U setuptools==59.5.0 Install JARVIS: pip install -e . With that out of the way the only thing left to do is downloading the example recordings by clicking here . Congratulations, you are all set up now! To launch our handy streamlit GUI interface just open a terminal, activate the conda environment by running conda activate jarvis and type jarvis launch . Alternatively you can also interact with jarvis through the command line. To do this activate the conda environment and then run jarvis launch-cli . The following sections give you the option to switch between instructions for both methods by selecting the respective tabs. 2. Visualizing the Example Trainingset Before we dive into training JARVIS to track anything it is always a good idea to have a look at the trainingset your are using, both in 2D and in 3D. GUI CLI To do this using the streamlit dashboard first launch the JARVIS streamlit dashboard as described above by running jarvis launch . Once the GUI pops up in your browser you can select the Example_Project from the drop-down menu and then navigate to the visualization menu. As you can see there are a bunch of option for visualizing both your predictions and your trainingset. You can see how that looks like above, but feel free to play around with it a bit to familiarize yourself with the data you are working with. To do this using the command line interface first launch it by running 'jarvis launch-cli' . You will see a menu appear in your terminal that you can navigate using your arrow keys. To visualize your dataset select the Visualize menu and then pick either the Dataset2D or the Dataset2D option. To visualize the example trainingset select the 'Example_Project' and the 'Hand' skeleton preset. Other than that feel free to play around with the different options.You can cycle through all the available frames by pressing any key. Pressing 'q' or 'esc' will take you back to the Visualize menu. Once you start working with your own data, checking your trainingset before training is really important to ensure there was no problem when creating it and your network will get the input you expect it to get. 3. Training the Entire Network Now that you know what our data looks like it is time to train the network stack. GUI CLI Using our GUI this is really easy, all you need to do is to navigate to the Train Full menu and press train as shown below. If everything works correctly you should see two progress bars as well as a plot showing the training progress appear. Depending on your GPU training might take up to a few hours, so a bit of patience is required at this point. If you don't want to wait you can also continue with our pretrained weights of course. The CLI makes this very easy. All you need to do is launch the interface by running jarvis launch-cli , select the Train menu and then run Train all as shown below. If everything works correctly you should see a progress bar appearing. Depending on your GPU training might take up to a few hours, so a bit of patience is required at this point. If you don't want to wait you can also continue with our pretrained weights of course. More Info on Network Training Our network stack is trained in four steps: Training CenterDetect: In this step a 2D-CNN is trained to detect the center of the entity you are tracking. This will be used to estimate the location of the entity in 3D, essentially telling the 3D-CNN where to look. Training KeypointDetect: In this step another 2D-CNN is trained to detect all your annotated keypoints in a single image. The output of this network will subsequently be used to construct the 3D feature volume that is the input of our 3D-CNN. Training HybridNet: In this step the 3D part of our full HybridNet architecture is trained. It's job is to use the 3D feature volumes created by the KeypointDetect stage to create the final 3D pose predictions. 4. Predicting Poses for the Example Recording If you haven't already you should now download our example recording . GUI CLI Once you have the example recording saved on your computer all you need to do is launch the JARVIS GUI and navigate to the Predict3D menu as shown below. Here you will have to specify a couple of things: Path of recording directory is the path of the example recording you just downloaded, it should include the 'Example_Recording' directory. Weights for CenterDetect / HybridNet lets you specify which weights you want to use. If you have trained models yourself you can leave them at 'latest'. If you didn't train the network yourself you will have to put the path of the pretrained weights here. They can be found in the 'pretrained' directory inside your 'JARVIS-Hybridnet' folder. Start Frame & Number Frames let you select on which part of the recording you want to run the prediction. For quick results set 'Number of Frames' to 1000. To predict until the end of the recording set it to -1. Once all those settings are correct, press the Predict button and wait for the progress bar to fill up as shown below. Once you have the example recording saved on your computer all you need to do is launch the JARVIS CLI and select Predict3D in the Predict menu as shown below. Here you will have to specify a couple of things: The Recordings Path is the path of the example recording you just downloaded, it should include the 'Example_Recording' directory. Select No for using TensorRT acceleration for now. If you installed the optional TensorRT packages this lets speed up predictions using NVIDIAs TensorRT library. Compiling the TRT models takes quite some time though. If you have trained models yourself you can use the most recently saved weights. Otherwise you will have to specify the path of the pretrained weights for the CenterDetect and the HybridNet networks here. They can be found in the 'pretrained' directory inside your 'JARVIS-Hybridnet' folder. Select No when asked if you want to use a calibration that is not in the trainingset . To quickly get some results also select No when asked wether you want to predict for the whole video Start Frame & Number of Frames let you select on which part of the recording you want to run the prediction. For quick results set 'Number of Frames' to 1000, to predict until the end of the recording set it to -1. After answering all the prompts you should see a progress bar filling up as shown below. Once the process is finished you will find a directory with a current timestamp in the projects folder under predictions . That folder contains a 'data3D.csv' file that contains the 3D coordinates and their corresponding confidences for every point in time. The directory also contains a '.yaml' file that holds some information necessary for creating videos from your predictions. 5. Creating Annotated Videos from Your Predictions The easiest way to check the quality of the predictions you just created is looking at annotated videos. For the 3D predictions those videos are created by projecting the 3D coordinates of the keypoints back into all available camera perspectives. GUI CLI In the GUI navigate to the Visualization menu as shown below. Here the right prediction directory should already be selected. If you want you can remove or add cameras from the list of cameras for which you want to create annotated videos. You can now click Create Video as shown below. If everything is set correctly you should find a directory containing your freshly labeled videos in the project directory after the progress bar is filled up. Navigate to the Visualize Menu after launching the JARVIS CLI. After selecting Create Videos 3D and the Example_Project you should be able to select the Predictions_3D directory that you created in the last step. If you want you can now select and deselect all the cameras that will be used to create your annotated videos. If everything is set correctly you should find a directory containing your freshly labeled videos in the project directory after the progress bar is filled up.","title":"2. Exploring the Example Trainingset"},{"location":"getting_started/2_exploring_example/#exploring-the-provided-example-trainingset","text":"Let's start by playing around with our provided example so you can familiarize with our software and get a better feel for the task and the workflow. The example data we're working with in this tutorial are recordings of one of our monkeys performing a simple grasping task in our 12 camera setup. Your task is to track his hand while he is enjoying a variety of fruits we hand him. We will split the task into four steps: Installing our Pytorch Toolbox and downloading the example recordings. Visualizing the provided annotations, both in 2D and 3D. Training the entire network stack. Predicting Poses for the Example Recording. Creating Annotated Videos from Your Predictions.","title":"Exploring the Provided Example Trainingset"},{"location":"getting_started/2_exploring_example/#1-installing-the-toolbox-and-downloading-the-data","text":"First let's take care of setting up the software. Make sure you have a version of Anaconda installed. If you want to train networks also make sure that your PC has a Nvidia GPU with working CUDA drivers installed. There are only a few simple steps you need to take to install the toolbox: - Download the python package. To do this open up a terminal and run: git clone https://github.com/JARVIS-MoCap/JARVIS-HybridNet.git && cd JARVIS-HybridNet Alternatively you download it directly by clicking here . Create the jarvis Anaconda environment by running: conda create -n jarvis python=3.9 pytorch=1.10.1 torchvision cudatoolkit=11.3 notebook -c pytorch Activate the environment (you will need to do this every time you open a terminal to use JARVIS): conda activate jarvis Install the required version of the setuptools package: pip install -U setuptools==59.5.0 Install JARVIS: pip install -e . With that out of the way the only thing left to do is downloading the example recordings by clicking here . Congratulations, you are all set up now! To launch our handy streamlit GUI interface just open a terminal, activate the conda environment by running conda activate jarvis and type jarvis launch . Alternatively you can also interact with jarvis through the command line. To do this activate the conda environment and then run jarvis launch-cli . The following sections give you the option to switch between instructions for both methods by selecting the respective tabs.","title":"1. Installing the Toolbox and Downloading the Data"},{"location":"getting_started/2_exploring_example/#2-visualizing-the-example-trainingset","text":"Before we dive into training JARVIS to track anything it is always a good idea to have a look at the trainingset your are using, both in 2D and in 3D. GUI CLI To do this using the streamlit dashboard first launch the JARVIS streamlit dashboard as described above by running jarvis launch . Once the GUI pops up in your browser you can select the Example_Project from the drop-down menu and then navigate to the visualization menu. As you can see there are a bunch of option for visualizing both your predictions and your trainingset. You can see how that looks like above, but feel free to play around with it a bit to familiarize yourself with the data you are working with. To do this using the command line interface first launch it by running 'jarvis launch-cli' . You will see a menu appear in your terminal that you can navigate using your arrow keys. To visualize your dataset select the Visualize menu and then pick either the Dataset2D or the Dataset2D option. To visualize the example trainingset select the 'Example_Project' and the 'Hand' skeleton preset. Other than that feel free to play around with the different options.You can cycle through all the available frames by pressing any key. Pressing 'q' or 'esc' will take you back to the Visualize menu. Once you start working with your own data, checking your trainingset before training is really important to ensure there was no problem when creating it and your network will get the input you expect it to get.","title":"2. Visualizing the Example Trainingset"},{"location":"getting_started/2_exploring_example/#3-training-the-entire-network","text":"Now that you know what our data looks like it is time to train the network stack. GUI CLI Using our GUI this is really easy, all you need to do is to navigate to the Train Full menu and press train as shown below. If everything works correctly you should see two progress bars as well as a plot showing the training progress appear. Depending on your GPU training might take up to a few hours, so a bit of patience is required at this point. If you don't want to wait you can also continue with our pretrained weights of course. The CLI makes this very easy. All you need to do is launch the interface by running jarvis launch-cli , select the Train menu and then run Train all as shown below. If everything works correctly you should see a progress bar appearing. Depending on your GPU training might take up to a few hours, so a bit of patience is required at this point. If you don't want to wait you can also continue with our pretrained weights of course. More Info on Network Training Our network stack is trained in four steps: Training CenterDetect: In this step a 2D-CNN is trained to detect the center of the entity you are tracking. This will be used to estimate the location of the entity in 3D, essentially telling the 3D-CNN where to look. Training KeypointDetect: In this step another 2D-CNN is trained to detect all your annotated keypoints in a single image. The output of this network will subsequently be used to construct the 3D feature volume that is the input of our 3D-CNN. Training HybridNet: In this step the 3D part of our full HybridNet architecture is trained. It's job is to use the 3D feature volumes created by the KeypointDetect stage to create the final 3D pose predictions.","title":"3. Training the Entire Network"},{"location":"getting_started/2_exploring_example/#4-predicting-poses-for-the-example-recording","text":"If you haven't already you should now download our example recording . GUI CLI Once you have the example recording saved on your computer all you need to do is launch the JARVIS GUI and navigate to the Predict3D menu as shown below. Here you will have to specify a couple of things: Path of recording directory is the path of the example recording you just downloaded, it should include the 'Example_Recording' directory. Weights for CenterDetect / HybridNet lets you specify which weights you want to use. If you have trained models yourself you can leave them at 'latest'. If you didn't train the network yourself you will have to put the path of the pretrained weights here. They can be found in the 'pretrained' directory inside your 'JARVIS-Hybridnet' folder. Start Frame & Number Frames let you select on which part of the recording you want to run the prediction. For quick results set 'Number of Frames' to 1000. To predict until the end of the recording set it to -1. Once all those settings are correct, press the Predict button and wait for the progress bar to fill up as shown below. Once you have the example recording saved on your computer all you need to do is launch the JARVIS CLI and select Predict3D in the Predict menu as shown below. Here you will have to specify a couple of things: The Recordings Path is the path of the example recording you just downloaded, it should include the 'Example_Recording' directory. Select No for using TensorRT acceleration for now. If you installed the optional TensorRT packages this lets speed up predictions using NVIDIAs TensorRT library. Compiling the TRT models takes quite some time though. If you have trained models yourself you can use the most recently saved weights. Otherwise you will have to specify the path of the pretrained weights for the CenterDetect and the HybridNet networks here. They can be found in the 'pretrained' directory inside your 'JARVIS-Hybridnet' folder. Select No when asked if you want to use a calibration that is not in the trainingset . To quickly get some results also select No when asked wether you want to predict for the whole video Start Frame & Number of Frames let you select on which part of the recording you want to run the prediction. For quick results set 'Number of Frames' to 1000, to predict until the end of the recording set it to -1. After answering all the prompts you should see a progress bar filling up as shown below. Once the process is finished you will find a directory with a current timestamp in the projects folder under predictions . That folder contains a 'data3D.csv' file that contains the 3D coordinates and their corresponding confidences for every point in time. The directory also contains a '.yaml' file that holds some information necessary for creating videos from your predictions.","title":"4. Predicting Poses for the Example Recording"},{"location":"getting_started/2_exploring_example/#5-creating-annotated-videos-from-your-predictions","text":"The easiest way to check the quality of the predictions you just created is looking at annotated videos. For the 3D predictions those videos are created by projecting the 3D coordinates of the keypoints back into all available camera perspectives. GUI CLI In the GUI navigate to the Visualization menu as shown below. Here the right prediction directory should already be selected. If you want you can remove or add cameras from the list of cameras for which you want to create annotated videos. You can now click Create Video as shown below. If everything is set correctly you should find a directory containing your freshly labeled videos in the project directory after the progress bar is filled up. Navigate to the Visualize Menu after launching the JARVIS CLI. After selecting Create Videos 3D and the Example_Project you should be able to select the Predictions_3D directory that you created in the last step. If you want you can now select and deselect all the cameras that will be used to create your annotated videos. If everything is set correctly you should find a directory containing your freshly labeled videos in the project directory after the progress bar is filled up.","title":"5. Creating Annotated Videos from Your Predictions"},{"location":"getting_started/3_creating_trainingset/","text":"Creating a New Trainingset from our Example Recordings Now that you know what a trainingset looks like and how you can use it to train the network we will take a step back and cover the process of creating this trainingset from a multi-camera recording. Like before we will split this task into smaller steps: Installing the AnnotationTool. Extracting a dataset from the Example Recording. Calibrating the Cameras. Annotating a Frameset. Exporting the dataset as a trainingset. If everything goes according to plan you will end up with a trainingset very similar to the one you used in the previous part of the tutorial. 1. Installing the AnnotationTool If you are using Windows, MacOS or Ubuntu all you have to do is got to our downloads page and grab the installer for your OS. If you are running a different Linux distribution you will have to build the AnnotationTool yourself. There is a guide on how to do that on its GitHub page . Once you install and launch the tool you will be greeted with a home screen that looks like this: 2. Extracting a Dataset from the Example Recording Clicking on the first item in the list on the homescreen will open up the dataset extraction menu. This will allow you to extract framesets from your recordings that you will subsequently annotate. For this tutorial we will stick to the basics and use the fastest and quickest way of extracting a handful of framesets. Definitely check out the relevant section in our Manual to learn all the details about how to create a dataset that is as representative of your entire recording as possible. First let's go through the options in the Configuration section. - New Dataset Name is the name of the dataset you're going to create. - New Dataset Path is the directory in which your new dataset will be saved. - Framesets to extract per Segment is the number of framesets the tool will create, you can leave it at the default of 10 for this example. - Sampling Method is the method the tool uses to decide which frames in your recording to use. kmeans is the method you should use for your real datasets, but for this tutorial we will use uniform to make the extraction process a bit quicker. Next up is the Recordings section. Just click Add Recording and navigate to the 'Example_Recording' directory and click Open . The last thing left to do is to tell the tool the names of all the Entities and Keypoints you want to annotate. Entities refers to the animal/object you are tracking. (Note: The AnnotationTool supports annotating multiple entities in one dataset, but the rest of our toolbox currently does not). For our example you can load the 'Hand' preset by clicking Load Preset . If your dataset menu looks like shown above you can click Create to create a new dataset. A progress window should pop up and once it is finished you should find a dataset folder containing a '.yaml' config file as well as a directory containing extracted frames for each camera. 3. Creating a Set of Calibration Parameters One of the most important steps in creating good 3D ground truth annotations is precise camera calibration. As always we have a comprehensive section on how to record calibration recordings in our Manual. For this example we provide a set of example calibration recordings that you can download by clicking here . Go back to the homescreen of the AnnotationTool and select the Create new Calibration menu item. Like before let's first go through the options in the General section. Calibration Set Name is the name of the set of calibration parameters you're going to create. Calibration Set Savepath is the directory in which your new calibration parameters will be saved. Separate Recordings for Intrinsics can be set to no if you want to use your extrinsic recordings for intrinsics calibration. This is not recommended, see the manual for details. Intrinsics Folder Path is the directory that contains all the recordings for intrinsic calibration (you will find it inside the downloaded 'Calibration_Example' directory). Extrinsics Folder Path is the directory that contains all the recordings for extrinsics calibration (you will find it inside the downloaded 'Calibration_Example' directory). Calibrating on your own data might require you to change some settings in the Calibration Settings and the Checkerboard Layout sections as well. For this example the defaults are already correct though, so you can leave all those settings untouched. Once you have entered all those settings you can click the Update Cameras button to add all cameras and pairs to the lists. After that you're almost ready to calibrate the cameras. The last thing you need to do is to delete the camera pair labeled 'Camera_LC --> Camera_B' and instead add the 3 camera triplet 'Camera_T --> Camera_LC --> Camera_B'. The GIF below shows you how to do that in detail. You're now ready to click the Calibrate Button . This will take a little bit, but once it is done a info window will pop up showing you the quality of all your calibrations. It should look similar to this: You will now find a directory containing one '.yaml' calibration file per camera at the path that you specified. 4. Annotating a Frameset Now that you have both a dataset and the calibrations that go along with it you can start annotating your framesets. First navigate back to the homescreen and select the Annotate Dataset item. Navigate to the directory of the dataset you created earlier and select the '.yaml' config file as shown below. You will then get an overview over the different segments of your dataset (in the example case there is only one) and a list of all your cameras. Select the segment you want to annotate and click the Load Dataset button. With the dataset loaded succesfully all that's left to do is to add the calibrations you created earlier to the dataset. To do this click on the big blue plus sign on the left side of the screen and select the directory that contains the '.yaml' calibration files. After that you can start annotating. The workflow is as following: 1. Annotate all the frames in one frameset. You can switch between them either by clicking the Next >> and << Previous on the left or by double clicking one of the cameras in the list. As soon as you annotate a keypoint in two or more cameras you will see a error bar appear in the Reprojection Tool . This is an indicator for the consistency of your annotations, the lower the better. 2. Switch to the next frameset once all reprojection error bars are sufficiently low. Once you annotated all joints for one frameset you can switch to the next one using the Next Set >> button. If you have a dataset consisting of more than one segment the dropdown in the top left corner will allow you to switch between segments. For a real dataset it is important that you annotate all framesets in a dataset before proceeding to the trainingset exporting step. Since this is only a tutorial we suggest you play around with the tool long enough to get familiar with and and get a feeling of how the Reprojection Tool works. Once you feel comfortable you can move on to the next step. 5. Exporting a Trainingset Almost done! As always navigate back to the homescreen and select the last item in the list: Export Trainingset . As before there are a handful of parameters you need to set: Trainingset Name is the name of the trainingset you will create. Trainingset Savepath is the directory the trainingset will be saved in. TrainingSet Type lets you select if you want to create a 2D or a 3D trainingset. 3D trainingsets include the calibration parameters and are what you almost always will be using. Only use the 2D option if you are working with single camera data or you are creating a pretraining trainingset. The rest of the parameters are related to how the data is split into training and validation data. The defaults should be fine for almost all applications, so just leave them untouched for the example. After setting your parameters you can click the blue 'plus' button to add one or more dataset to the trainingset. For the example add only the dataset you did annotate earlier. Selection works just like in the annotation mode by selecting the '.yaml' dataset config file. After adding the dataset the pie chart in the bottom left corner should be show some statistics about your dataset. Since we did not annotate many frames the majority of keypoints will be unnanotated. If everything looks like shown below you can click the Create Trainingset button. You should now have a trainingset that has the same structure as the one you used to train your first network. That's it! Now it's time to get started with training a model on your own data. If you want to learn more about our toolbox we strongly suggest you have a look at our Manual . There you will find detailed instructions on every step of building a 3D motion capture setup with JARVIS.","title":"3. Creating a new Trainingset"},{"location":"getting_started/3_creating_trainingset/#creating-a-new-trainingset-from-our-example-recordings","text":"Now that you know what a trainingset looks like and how you can use it to train the network we will take a step back and cover the process of creating this trainingset from a multi-camera recording. Like before we will split this task into smaller steps: Installing the AnnotationTool. Extracting a dataset from the Example Recording. Calibrating the Cameras. Annotating a Frameset. Exporting the dataset as a trainingset. If everything goes according to plan you will end up with a trainingset very similar to the one you used in the previous part of the tutorial.","title":"Creating a New Trainingset from our Example Recordings"},{"location":"getting_started/3_creating_trainingset/#1-installing-the-annotationtool","text":"If you are using Windows, MacOS or Ubuntu all you have to do is got to our downloads page and grab the installer for your OS. If you are running a different Linux distribution you will have to build the AnnotationTool yourself. There is a guide on how to do that on its GitHub page . Once you install and launch the tool you will be greeted with a home screen that looks like this:","title":"1. Installing the AnnotationTool"},{"location":"getting_started/3_creating_trainingset/#2-extracting-a-dataset-from-the-example-recording","text":"Clicking on the first item in the list on the homescreen will open up the dataset extraction menu. This will allow you to extract framesets from your recordings that you will subsequently annotate. For this tutorial we will stick to the basics and use the fastest and quickest way of extracting a handful of framesets. Definitely check out the relevant section in our Manual to learn all the details about how to create a dataset that is as representative of your entire recording as possible. First let's go through the options in the Configuration section. - New Dataset Name is the name of the dataset you're going to create. - New Dataset Path is the directory in which your new dataset will be saved. - Framesets to extract per Segment is the number of framesets the tool will create, you can leave it at the default of 10 for this example. - Sampling Method is the method the tool uses to decide which frames in your recording to use. kmeans is the method you should use for your real datasets, but for this tutorial we will use uniform to make the extraction process a bit quicker. Next up is the Recordings section. Just click Add Recording and navigate to the 'Example_Recording' directory and click Open . The last thing left to do is to tell the tool the names of all the Entities and Keypoints you want to annotate. Entities refers to the animal/object you are tracking. (Note: The AnnotationTool supports annotating multiple entities in one dataset, but the rest of our toolbox currently does not). For our example you can load the 'Hand' preset by clicking Load Preset . If your dataset menu looks like shown above you can click Create to create a new dataset. A progress window should pop up and once it is finished you should find a dataset folder containing a '.yaml' config file as well as a directory containing extracted frames for each camera.","title":"2. Extracting a Dataset from the Example Recording"},{"location":"getting_started/3_creating_trainingset/#3-creating-a-set-of-calibration-parameters","text":"One of the most important steps in creating good 3D ground truth annotations is precise camera calibration. As always we have a comprehensive section on how to record calibration recordings in our Manual. For this example we provide a set of example calibration recordings that you can download by clicking here . Go back to the homescreen of the AnnotationTool and select the Create new Calibration menu item. Like before let's first go through the options in the General section. Calibration Set Name is the name of the set of calibration parameters you're going to create. Calibration Set Savepath is the directory in which your new calibration parameters will be saved. Separate Recordings for Intrinsics can be set to no if you want to use your extrinsic recordings for intrinsics calibration. This is not recommended, see the manual for details. Intrinsics Folder Path is the directory that contains all the recordings for intrinsic calibration (you will find it inside the downloaded 'Calibration_Example' directory). Extrinsics Folder Path is the directory that contains all the recordings for extrinsics calibration (you will find it inside the downloaded 'Calibration_Example' directory). Calibrating on your own data might require you to change some settings in the Calibration Settings and the Checkerboard Layout sections as well. For this example the defaults are already correct though, so you can leave all those settings untouched. Once you have entered all those settings you can click the Update Cameras button to add all cameras and pairs to the lists. After that you're almost ready to calibrate the cameras. The last thing you need to do is to delete the camera pair labeled 'Camera_LC --> Camera_B' and instead add the 3 camera triplet 'Camera_T --> Camera_LC --> Camera_B'. The GIF below shows you how to do that in detail. You're now ready to click the Calibrate Button . This will take a little bit, but once it is done a info window will pop up showing you the quality of all your calibrations. It should look similar to this: You will now find a directory containing one '.yaml' calibration file per camera at the path that you specified.","title":"3. Creating a Set of Calibration Parameters"},{"location":"getting_started/3_creating_trainingset/#4-annotating-a-frameset","text":"Now that you have both a dataset and the calibrations that go along with it you can start annotating your framesets. First navigate back to the homescreen and select the Annotate Dataset item. Navigate to the directory of the dataset you created earlier and select the '.yaml' config file as shown below. You will then get an overview over the different segments of your dataset (in the example case there is only one) and a list of all your cameras. Select the segment you want to annotate and click the Load Dataset button. With the dataset loaded succesfully all that's left to do is to add the calibrations you created earlier to the dataset. To do this click on the big blue plus sign on the left side of the screen and select the directory that contains the '.yaml' calibration files. After that you can start annotating. The workflow is as following: 1. Annotate all the frames in one frameset. You can switch between them either by clicking the Next >> and << Previous on the left or by double clicking one of the cameras in the list. As soon as you annotate a keypoint in two or more cameras you will see a error bar appear in the Reprojection Tool . This is an indicator for the consistency of your annotations, the lower the better. 2. Switch to the next frameset once all reprojection error bars are sufficiently low. Once you annotated all joints for one frameset you can switch to the next one using the Next Set >> button. If you have a dataset consisting of more than one segment the dropdown in the top left corner will allow you to switch between segments. For a real dataset it is important that you annotate all framesets in a dataset before proceeding to the trainingset exporting step. Since this is only a tutorial we suggest you play around with the tool long enough to get familiar with and and get a feeling of how the Reprojection Tool works. Once you feel comfortable you can move on to the next step.","title":"4. Annotating a Frameset"},{"location":"getting_started/3_creating_trainingset/#5-exporting-a-trainingset","text":"Almost done! As always navigate back to the homescreen and select the last item in the list: Export Trainingset . As before there are a handful of parameters you need to set: Trainingset Name is the name of the trainingset you will create. Trainingset Savepath is the directory the trainingset will be saved in. TrainingSet Type lets you select if you want to create a 2D or a 3D trainingset. 3D trainingsets include the calibration parameters and are what you almost always will be using. Only use the 2D option if you are working with single camera data or you are creating a pretraining trainingset. The rest of the parameters are related to how the data is split into training and validation data. The defaults should be fine for almost all applications, so just leave them untouched for the example. After setting your parameters you can click the blue 'plus' button to add one or more dataset to the trainingset. For the example add only the dataset you did annotate earlier. Selection works just like in the annotation mode by selecting the '.yaml' dataset config file. After adding the dataset the pie chart in the bottom left corner should be show some statistics about your dataset. Since we did not annotate many frames the majority of keypoints will be unnanotated. If everything looks like shown below you can click the Create Trainingset button. You should now have a trainingset that has the same structure as the one you used to train your first network. That's it! Now it's time to get started with training a model on your own data. If you want to learn more about our toolbox we strongly suggest you have a look at our Manual . There you will find detailed instructions on every step of building a 3D motion capture setup with JARVIS.","title":"5. Exporting a Trainingset"},{"location":"manual/1_introduction/","text":"JARVIS Manual This Manual is very much work in progress, please check back in a couple of weeks if the part you are interested in is not covered yet! This Manual covers all the basic steps you need to take to get from the idea of using markerless tracking to 3D pose predictions ready for analysis. This obviously makes it quite a lengthy read, so feel free to skip any sections that might not be relevant to you. The blue Troubleshooting boxes can be expanded by clicking on them and will contain some hopefully helpful hints if you encounter any issues.","title":"1. Introduction"},{"location":"manual/1_introduction/#jarvis-manual","text":"This Manual is very much work in progress, please check back in a couple of weeks if the part you are interested in is not covered yet! This Manual covers all the basic steps you need to take to get from the idea of using markerless tracking to 3D pose predictions ready for analysis. This obviously makes it quite a lengthy read, so feel free to skip any sections that might not be relevant to you. The blue Troubleshooting boxes can be expanded by clicking on them and will contain some hopefully helpful hints if you encounter any issues.","title":"JARVIS Manual"},{"location":"manual/2_mocap_setup_design/","text":"Designing a 3D Motion Capture Setup Designing a good Camera Setup for Motion Capture is probably the most critical step to achieve reliable tracking and will safe you a lot of frustration while annotating, training your network and analyzing your data.\\ That being said, without a proper starting point the vast number of available cameras, lenses, lighting options and possible setup configurations can be quite overwhelming. That's why your first step should be to answer the following questions for your planned setup. The first set of questions will cover all the camera and lens specific decisions you'll have to make. Keep in mind that if you want to use our Acquisition Tool without any modifications you should use a camera model from our list of supported cameras . What is the size of the area my subject will move in and at what distance do I want to mount my cameras? This will determine the field of view and with that the focal length of your lenses. You can use this handy Calculator to help you pick the correct values. If your setup requires lenses with a very short focal length, make sure to get lenses with as little distortion as possible. What is the size of the smallest features you want to resolve? This will determine the resolution of your camera sensor. Make sure you are able to annotate all the keypoints you want without to much difficulty at the resolution you choose. What is the fastest speed my subject will move at? This will determine both the minimum frame rate you need to record at and the maximum exposure time you can use. As a rough rule of thumb a framerate of 50 Hz works well for everyting that moves at human speed or lower, for fast moving subjects like monkeys or mice you'll want to go up to at least 100 Hz. If you want a more rigorous rule the Nyquist Theorem is your friend. How long will the cables going from my cameras to the recording computer be? This will determine whether GigE or USB3.0 cameras are the better choice for you. We generally recommend using USB cameras, but for distances longer than a couple of meters the ethernet based GigE cameras are the better choice. Now that the configuration of the individual cameras is out of the way, we can move on to the most important question regarding the whole setup. How precise does the tracking need to be for my application and how much occlusion (both by other objects in the setup and by the subject itself) do I expect? This is the most important question as it determines the number of cameras you will need to use. First of all think about how many cameras fit into your budget keeping in mind that more cameras also require a more powerful recording computer. You can then determine the absolute minimum number of cameras that you need by making sure that every keypoint you want to track is visible in at least two cameras at all times, as illustrated in the sketch below. Note that this is the absolute minimum and depending on the precision you require we recommend to have every keypoint visible in at least three or four cameras at all times. While thinking about your camera configuration try to make the angles between the cameras as wide as possible. Ideally you want to distribute your cameras as evenly as possible on a sphere around your tracking volume. The sketch below again tries to illustrate that principle. With that all of the basic design decisions should be covered and the only thing left is a list of some of the easily overlooked but still very important things to consider: There are two ways to go about mounting your cameras. You can either build a very rigid and permanent mounting system, or one that is flexible and easily repositioned. Both have their obvious advantages and disadvantages and you need to decide what fits your setup structure best. But keep in mind that by going for a flexible system you will have to recalibrate your setup EVERY time you use it. Accurate calibrations are the foundation of precise 3D tracking and its hard to stress its importance enough. Make sure you design your setup in a way that allows you to record good calibration videos. You can check out our example calibration recording to get an idea of how that looks like. To close of this section, here is a render of the basic structure of our 12 camera hand-tracking setup, incase you need some inspiration:","title":"2. Designing a Setup"},{"location":"manual/2_mocap_setup_design/#designing-a-3d-motion-capture-setup","text":"Designing a good Camera Setup for Motion Capture is probably the most critical step to achieve reliable tracking and will safe you a lot of frustration while annotating, training your network and analyzing your data.\\ That being said, without a proper starting point the vast number of available cameras, lenses, lighting options and possible setup configurations can be quite overwhelming. That's why your first step should be to answer the following questions for your planned setup. The first set of questions will cover all the camera and lens specific decisions you'll have to make. Keep in mind that if you want to use our Acquisition Tool without any modifications you should use a camera model from our list of supported cameras . What is the size of the area my subject will move in and at what distance do I want to mount my cameras? This will determine the field of view and with that the focal length of your lenses. You can use this handy Calculator to help you pick the correct values. If your setup requires lenses with a very short focal length, make sure to get lenses with as little distortion as possible. What is the size of the smallest features you want to resolve? This will determine the resolution of your camera sensor. Make sure you are able to annotate all the keypoints you want without to much difficulty at the resolution you choose. What is the fastest speed my subject will move at? This will determine both the minimum frame rate you need to record at and the maximum exposure time you can use. As a rough rule of thumb a framerate of 50 Hz works well for everyting that moves at human speed or lower, for fast moving subjects like monkeys or mice you'll want to go up to at least 100 Hz. If you want a more rigorous rule the Nyquist Theorem is your friend. How long will the cables going from my cameras to the recording computer be? This will determine whether GigE or USB3.0 cameras are the better choice for you. We generally recommend using USB cameras, but for distances longer than a couple of meters the ethernet based GigE cameras are the better choice. Now that the configuration of the individual cameras is out of the way, we can move on to the most important question regarding the whole setup. How precise does the tracking need to be for my application and how much occlusion (both by other objects in the setup and by the subject itself) do I expect? This is the most important question as it determines the number of cameras you will need to use. First of all think about how many cameras fit into your budget keeping in mind that more cameras also require a more powerful recording computer. You can then determine the absolute minimum number of cameras that you need by making sure that every keypoint you want to track is visible in at least two cameras at all times, as illustrated in the sketch below. Note that this is the absolute minimum and depending on the precision you require we recommend to have every keypoint visible in at least three or four cameras at all times. While thinking about your camera configuration try to make the angles between the cameras as wide as possible. Ideally you want to distribute your cameras as evenly as possible on a sphere around your tracking volume. The sketch below again tries to illustrate that principle. With that all of the basic design decisions should be covered and the only thing left is a list of some of the easily overlooked but still very important things to consider: There are two ways to go about mounting your cameras. You can either build a very rigid and permanent mounting system, or one that is flexible and easily repositioned. Both have their obvious advantages and disadvantages and you need to decide what fits your setup structure best. But keep in mind that by going for a flexible system you will have to recalibrate your setup EVERY time you use it. Accurate calibrations are the foundation of precise 3D tracking and its hard to stress its importance enough. Make sure you design your setup in a way that allows you to record good calibration videos. You can check out our example calibration recording to get an idea of how that looks like. To close of this section, here is a render of the basic structure of our 12 camera hand-tracking setup, incase you need some inspiration:","title":"Designing a 3D Motion Capture Setup"},{"location":"manual/3_acquisitiontool_setup/","text":"Setting up the AcquisitionTool With the basic setup design out of the way, the next challenge to tackle is getting all your cameras to record synchronized videos for you. Our AcquisitionTool makes this process very easy for all FLIR machine vision cameras . 1. Hardware Requirements A set of FLIR cameras: We recommend the BlackFly S model, but choose whatever fits your application best. Matching GPIO cables: Those are required to hook up the external trigger, you can order them with your cameras on the FLIR website. An Arduino Uno (or similar): This will be used as the source of the external trigger (And as a way to control and monitor your setup in future releases). A recording computer with the following specs: A proper lighting solution: This one really depends on your setup, so we can't give any exact recommendations. But make sure you don't forget to take proper lighting into account when designing your setup. The most important factors here are even illumination from all sides and overall brightness. A recent Nvidia GPU: 10xx, 20xx and 30xx series cards will work. If you are using more eight cameras or more we recommend at least a 2080 or preferably a 30-series card. A decent CPU Our tool offloads most of the work to the GPU, a modern Intel i7 or equivalent is strongly recommended. A fast SSD Even with compression you are still writing a lot of data. We recommend SSDs with a write speed of at least 3000 MB/s. Enough USB Ports: Make sure you have enough available USB3 ports if you are using USB cameras. FLIR sells some suitable USB Host Controller Cards . Be aware of USB bandwidth sharing when using motherboard usb ports Caution: Some USB ports on your computer can share their bandwidth, which will cause issues if you are recording at high resolutions. You can check that in the spec sheet of your motherboard or the Hardware check tab in the AcquisitionTool Settings . 2. Software Installation The first thing you will have to do is install the FLIR Spinnaker SDK . You can download it here . If you are running Windows make sure to download the '*_x64.exe' found in the 'Latest Spinnaker Full SDK' directory. For Linux the '*amd64-pkg.tar.gz' is the package you want. Once that is installed you can grab the AcquisitionTool Installer from the Downloads section. Under Windows just run the installer and follow the instructions. Under Linux you can install the AcquisitionTool by running sudo apt install ./JARVIS-AcquisitionTool_1.0-1_amd64_2004.deb (Make sure to replace the version numbers with the version you downloaded). If the installation completed successfully the AcquisitionTool should now be available in your Start menu under Windows. If you are running Linux you can open it by typing AcquisitionTool into a terminal and pressing enter. This is a good time to test out if everything is working as intended, before we move on to setting up synchronization with the external trigger.\\ For a quick test connect at least one or two cameras to your computer, launch the AcquisitionTool and navigate to the Connection mode as shown below. You can now either connect each camera individually by clicking one of the slots or simply detect all cameras with the Auto Detect Cameras button. If that works you can then switch back to the Acquisition Mode. If your cameras are in their default mode you should be able to get them streaming by clicking the button in the top left corner. If you now see live images of what all your cameras are seeing you are set to move on to the next and final setup step for the AcquisitionTool. Troubleshooting and Hints Cameras not connecting: If your cameras do not show up after clicking the Auto Detect button there's a couple of things you can do: Make sure your camera is plugged into a (not shared) USB3 port (Usually the ones with the blue plastic part). Check whether you are able to access them in SpinView. If that also fails try reinstalling the Spinnaker drivers. Linux only: Make sure you did setup the group permissions and the USB buffer size correctly. Cameras not streaming: If any of your cameras are not showing an image after you start streaming this is very likely due to them being set up in a non standard acquisition mode. To make sure they have their default settings loaded you can follow the these steps: Select a camera by double clicking its name in the list in the top left corner Click the arrow in the top right corner of the 'Camera Settings' Tab. This should open up the presets menu. In the menu select the 'Default' UserSet by clicking on it and press 'Load'. Try streaming again by clicking the green button, it should now work for the camera you selected. Repeat those steps for all cameras that are not showing an image. Building the Tool from source: If you are planning on using the AcquisitionTool on a different Linux distribution or are interested in modifying the source code check out the GitHub Repo ! It has detailed instructions on how to build the tool and its dependencies yourself. 3. Setting up the External Trigger At this point you should have a recording setup that can be controlled using our AcquisitonTool and can stream video from all of your cameras. The last but very important step that is still missing is making sure all cameras record their videos perfectly in sync. To do this we use an external trigger pulse supplied by an Arduino Uno (or similar, our PlatformIO project supports many of the commonly used Microcontrollers). Programming the Arduino Programming the Arduino is really easy thanks to our PlatformIO install scripts for both Linux and Windows. Simply do the following: Linux Windows Clone our TriggerFirmware Repository from GitHub with: git clone --recursive https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git Change into the TriggerFirmware directory and execute the install script: cd JARVIS-TriggerFirmware && sh install_arduino_uno.sh Make sure you have a recent version of Python installed. You can either get it directly from the Microsoft Store or download it from here . Clone our TriggerFirmware Repository from GitHub with: git clone --recursive https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git Do not use the Download ZIP option on GitHub GitHub does not include all neccessary submodules in its .zip download. If you don't have git installed on your computer you can click here to download a .zip folder containing all the necessary data. Change into the TriggerFirmware directory and execute the installer batch file: .\\install_arduino_uno.bat If the install throws an error related to Long Path Support first remove the JARVIS-TriggerFirmware\\PlatformIO\\install directory and then open a command prompt as administrator and run: reg add \"HKLM\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v LongPathsEnabled /t REG_DWORD /d 1 Wiring up the Arduino Now comes the slightly tricky part. wiring up all the trigger cables. The exact connections you have to make depend on your exact camera model, but the general idea is always the same: Connect the ground (GND) pins of all cameras to one of the pins on the Arduino labeled GND. Connect the trigger input (check your cameras datasheet) pin of all cameras to pin 6 of your Arduino. For more details on how to hook up your specific camera this guide from FLIR might be helpful. Here is a very basic wiring diagram for the FLIR BlackFly S: Once your Arduino trigger system is all wired up you can go back to the AcquisitionTool and connect the trigger using the button. To make the cameras use the trigger signal there are a few settings you will have to change on each camera. Again, the exact settings might vary slightly depending on your camera model. This guide shows the settings for the BlackFly S, check FLIRs documentation on your camera model if those settings don't work. Here's the step-by-step guide: Select a camera by double clicking its name in the list in the top left corner Make sure the cameras default settings are loaded: Click the arrow in the top right corner of the 'Camera Settings' Tab. This should open up the presets menu. In the menu select the 'Default' UserSet by clicking on it and press 'Load'. Click the button to get access to all settings. Here's all the settings you will have to change: Trigger Mode: On Trigger Source: Line3 Trigger Overlap: ReadOut Pixel Format: BayerRG8 Exposure Auto: Off Exposure Time: Needs to be shorter than the time between your frames (At 100 FPS the limit is 10 ms). Caution: The value is set in microseconds! With all those settings adjusted you should now be able to record synchronized videos! To be sure it's best to do a test recording and checking if all videos have exactly the same length.","title":"3. Setting up the AcquisitionTool"},{"location":"manual/3_acquisitiontool_setup/#setting-up-the-acquisitiontool","text":"With the basic setup design out of the way, the next challenge to tackle is getting all your cameras to record synchronized videos for you. Our AcquisitionTool makes this process very easy for all FLIR machine vision cameras .","title":"Setting up the AcquisitionTool"},{"location":"manual/3_acquisitiontool_setup/#1-hardware-requirements","text":"A set of FLIR cameras: We recommend the BlackFly S model, but choose whatever fits your application best. Matching GPIO cables: Those are required to hook up the external trigger, you can order them with your cameras on the FLIR website. An Arduino Uno (or similar): This will be used as the source of the external trigger (And as a way to control and monitor your setup in future releases). A recording computer with the following specs: A proper lighting solution: This one really depends on your setup, so we can't give any exact recommendations. But make sure you don't forget to take proper lighting into account when designing your setup. The most important factors here are even illumination from all sides and overall brightness. A recent Nvidia GPU: 10xx, 20xx and 30xx series cards will work. If you are using more eight cameras or more we recommend at least a 2080 or preferably a 30-series card. A decent CPU Our tool offloads most of the work to the GPU, a modern Intel i7 or equivalent is strongly recommended. A fast SSD Even with compression you are still writing a lot of data. We recommend SSDs with a write speed of at least 3000 MB/s. Enough USB Ports: Make sure you have enough available USB3 ports if you are using USB cameras. FLIR sells some suitable USB Host Controller Cards . Be aware of USB bandwidth sharing when using motherboard usb ports Caution: Some USB ports on your computer can share their bandwidth, which will cause issues if you are recording at high resolutions. You can check that in the spec sheet of your motherboard or the Hardware check tab in the AcquisitionTool Settings .","title":"1. Hardware Requirements"},{"location":"manual/3_acquisitiontool_setup/#2-software-installation","text":"The first thing you will have to do is install the FLIR Spinnaker SDK . You can download it here . If you are running Windows make sure to download the '*_x64.exe' found in the 'Latest Spinnaker Full SDK' directory. For Linux the '*amd64-pkg.tar.gz' is the package you want. Once that is installed you can grab the AcquisitionTool Installer from the Downloads section. Under Windows just run the installer and follow the instructions. Under Linux you can install the AcquisitionTool by running sudo apt install ./JARVIS-AcquisitionTool_1.0-1_amd64_2004.deb (Make sure to replace the version numbers with the version you downloaded). If the installation completed successfully the AcquisitionTool should now be available in your Start menu under Windows. If you are running Linux you can open it by typing AcquisitionTool into a terminal and pressing enter. This is a good time to test out if everything is working as intended, before we move on to setting up synchronization with the external trigger.\\ For a quick test connect at least one or two cameras to your computer, launch the AcquisitionTool and navigate to the Connection mode as shown below. You can now either connect each camera individually by clicking one of the slots or simply detect all cameras with the Auto Detect Cameras button. If that works you can then switch back to the Acquisition Mode. If your cameras are in their default mode you should be able to get them streaming by clicking the button in the top left corner. If you now see live images of what all your cameras are seeing you are set to move on to the next and final setup step for the AcquisitionTool. Troubleshooting and Hints Cameras not connecting: If your cameras do not show up after clicking the Auto Detect button there's a couple of things you can do: Make sure your camera is plugged into a (not shared) USB3 port (Usually the ones with the blue plastic part). Check whether you are able to access them in SpinView. If that also fails try reinstalling the Spinnaker drivers. Linux only: Make sure you did setup the group permissions and the USB buffer size correctly. Cameras not streaming: If any of your cameras are not showing an image after you start streaming this is very likely due to them being set up in a non standard acquisition mode. To make sure they have their default settings loaded you can follow the these steps: Select a camera by double clicking its name in the list in the top left corner Click the arrow in the top right corner of the 'Camera Settings' Tab. This should open up the presets menu. In the menu select the 'Default' UserSet by clicking on it and press 'Load'. Try streaming again by clicking the green button, it should now work for the camera you selected. Repeat those steps for all cameras that are not showing an image. Building the Tool from source: If you are planning on using the AcquisitionTool on a different Linux distribution or are interested in modifying the source code check out the GitHub Repo ! It has detailed instructions on how to build the tool and its dependencies yourself.","title":"2. Software Installation"},{"location":"manual/3_acquisitiontool_setup/#3-setting-up-the-external-trigger","text":"At this point you should have a recording setup that can be controlled using our AcquisitonTool and can stream video from all of your cameras. The last but very important step that is still missing is making sure all cameras record their videos perfectly in sync. To do this we use an external trigger pulse supplied by an Arduino Uno (or similar, our PlatformIO project supports many of the commonly used Microcontrollers).","title":"3. Setting up the External Trigger"},{"location":"manual/3_acquisitiontool_setup/#programming-the-arduino","text":"Programming the Arduino is really easy thanks to our PlatformIO install scripts for both Linux and Windows. Simply do the following: Linux Windows Clone our TriggerFirmware Repository from GitHub with: git clone --recursive https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git Change into the TriggerFirmware directory and execute the install script: cd JARVIS-TriggerFirmware && sh install_arduino_uno.sh Make sure you have a recent version of Python installed. You can either get it directly from the Microsoft Store or download it from here . Clone our TriggerFirmware Repository from GitHub with: git clone --recursive https://github.com/JARVIS-MoCap/JARVIS-TriggerFirmware.git Do not use the Download ZIP option on GitHub GitHub does not include all neccessary submodules in its .zip download. If you don't have git installed on your computer you can click here to download a .zip folder containing all the necessary data. Change into the TriggerFirmware directory and execute the installer batch file: .\\install_arduino_uno.bat If the install throws an error related to Long Path Support first remove the JARVIS-TriggerFirmware\\PlatformIO\\install directory and then open a command prompt as administrator and run: reg add \"HKLM\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v LongPathsEnabled /t REG_DWORD /d 1","title":"Programming the Arduino"},{"location":"manual/3_acquisitiontool_setup/#wiring-up-the-arduino","text":"Now comes the slightly tricky part. wiring up all the trigger cables. The exact connections you have to make depend on your exact camera model, but the general idea is always the same: Connect the ground (GND) pins of all cameras to one of the pins on the Arduino labeled GND. Connect the trigger input (check your cameras datasheet) pin of all cameras to pin 6 of your Arduino. For more details on how to hook up your specific camera this guide from FLIR might be helpful. Here is a very basic wiring diagram for the FLIR BlackFly S: Once your Arduino trigger system is all wired up you can go back to the AcquisitionTool and connect the trigger using the button. To make the cameras use the trigger signal there are a few settings you will have to change on each camera. Again, the exact settings might vary slightly depending on your camera model. This guide shows the settings for the BlackFly S, check FLIRs documentation on your camera model if those settings don't work. Here's the step-by-step guide: Select a camera by double clicking its name in the list in the top left corner Make sure the cameras default settings are loaded: Click the arrow in the top right corner of the 'Camera Settings' Tab. This should open up the presets menu. In the menu select the 'Default' UserSet by clicking on it and press 'Load'. Click the button to get access to all settings. Here's all the settings you will have to change: Trigger Mode: On Trigger Source: Line3 Trigger Overlap: ReadOut Pixel Format: BayerRG8 Exposure Auto: Off Exposure Time: Needs to be shorter than the time between your frames (At 100 FPS the limit is 10 ms). Caution: The value is set in microseconds! With all those settings adjusted you should now be able to record synchronized videos! To be sure it's best to do a test recording and checking if all videos have exactly the same length.","title":"Wiring up the Arduino"},{"location":"manual/4_recording_calibration/","text":"Recording Good Calibration Videos Intrinsics Calibration The first step to get your camera calibration files is to record one calibration video for each camera. This video will be used to compute its focal length, principal point offset and distortion parameters. Or in simpler terms: all the camera specific parameters that we need for 3D reconstruction. There are a few rules you have to follow to get the best intrinsics calibration possible: Move the checkerboard along all axis (especially rotation!) Make sure you do not only record frames with the checkerboard parallel to the camera (as shown in the picture on the left). Not rotating it enough makes it impossible for the calibration tool to estimate the focal length correctly. Fill as much of the field of view with the checkerboard as possible Make sure to be close enough to the camera to take advantage of the full resolution of your camera to cover at least 2/3 of the cameras field of view. Obvioulsy it's just as important to not get so close the camera looses focus, a bigger checkerboard will help in those cases. Extrinsics Calibration The second step is to select a primary camera. This camera defines your reference frame when doing 3D reconstruction. To calibrate the extrinsic parameters you will have to record videos for all possible camera pairs that contain the primary camera. Those videos will be used to calculate the position of all secondary cameras relative to the primary camera. The main thing to watch out for during extrinsics recordings is that both cameras have a good and unoccluded view of the checkerboard. As long as that's the case it is as simple as just waving the board around as much as you can.\\ Side note: While it is strongly recommended to record seperate videos for intrinsics and extrinsics it is possibly to use your extrinsic recordings for intrinsics calibration. If you do so make sure to still follow the intrinsic calibration rules during your recordings. The main thing to watch out for during extrinsics recordings is that both cameras have a good and unoccluded view of the checkerboard. As long as that's the case it is as simple as just waving the board around as much as you can.\\","title":"4. Recording Calibration Videos"},{"location":"manual/4_recording_calibration/#recording-good-calibration-videos","text":"","title":"Recording Good Calibration Videos"},{"location":"manual/4_recording_calibration/#intrinsics-calibration","text":"The first step to get your camera calibration files is to record one calibration video for each camera. This video will be used to compute its focal length, principal point offset and distortion parameters. Or in simpler terms: all the camera specific parameters that we need for 3D reconstruction. There are a few rules you have to follow to get the best intrinsics calibration possible: Move the checkerboard along all axis (especially rotation!) Make sure you do not only record frames with the checkerboard parallel to the camera (as shown in the picture on the left). Not rotating it enough makes it impossible for the calibration tool to estimate the focal length correctly. Fill as much of the field of view with the checkerboard as possible Make sure to be close enough to the camera to take advantage of the full resolution of your camera to cover at least 2/3 of the cameras field of view. Obvioulsy it's just as important to not get so close the camera looses focus, a bigger checkerboard will help in those cases.","title":"Intrinsics Calibration"},{"location":"manual/4_recording_calibration/#extrinsics-calibration","text":"The second step is to select a primary camera. This camera defines your reference frame when doing 3D reconstruction. To calibrate the extrinsic parameters you will have to record videos for all possible camera pairs that contain the primary camera. Those videos will be used to calculate the position of all secondary cameras relative to the primary camera. The main thing to watch out for during extrinsics recordings is that both cameras have a good and unoccluded view of the checkerboard. As long as that's the case it is as simple as just waving the board around as much as you can.\\ Side note: While it is strongly recommended to record seperate videos for intrinsics and extrinsics it is possibly to use your extrinsic recordings for intrinsics calibration. If you do so make sure to still follow the intrinsic calibration rules during your recordings. The main thing to watch out for during extrinsics recordings is that both cameras have a good and unoccluded view of the checkerboard. As long as that's the case it is as simple as just waving the board around as much as you can.\\","title":"Extrinsics Calibration"},{"location":"manual/5_creating_and_labeling_datasts/","text":"Creating and Labeling Datasets","title":"5. Creating and Labeling Datasets"},{"location":"manual/5_creating_and_labeling_datasts/#creating-and-labeling-datasets","text":"","title":"Creating and Labeling Datasets"},{"location":"manual/6_synchronization/","text":"How to synchronize your devices If you use our external trigger, you can use it's inputs, to synchronize your devices. Most of the supported microcontroller boards log the inputs at pins 16-23 into a /triggerdata.csv file. For time synchronization, the microcontroller boards logs the first inputs at the time the first pulse was sent. This way you can synchronize the first images of the cameras and their internal clock to the internal clock of the external trigger. \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500 \u2500\u2510 ... \u2502 \u2502 \u2502 \u2502 \u2502 ... \u2502 ... \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 wavestate 0 \u2502 1 \u2502 0 \u2502 1 \u2502 0 \u2502 1 n \u2502 0 pulse_count UINT32_MAX \u2502 0 \u2502 0 \u2502 1 \u2502 1 \u2502 2 n \u2502 UINT32_MAX pulse_count (cont.) n \u2502 n+1 \u2502 n+1 \u2502 n+2 \u2502 n+2 \u2502n+3 n+m\u2502 n+m sync_rising_edge == true \u2534 \u2502 sync_rising_edge == false \u2534 The file contents will look similar to this: metadata.csv frame_camera_serial ;frame_camera_name ;frame_id ;frame_timestamp ;frame_image_uid 19415034 ;Camera_4 ; 0 ;597449368203896 ; 94017979513872 19415037 ;Camera_3 ; 0 ;163273368203367 ; 94017967892976 19415033 ;Camera_1 ; 0 ;272583328921019 ; 94017972152896 21013374 ;Camera_0 ; 0 ;770316960921060 ; 94017962792256 19415035 ;Camera_2 ; 0 ;855758496921019 ; 94017968796096 19415037 ;Camera_3 ; 1 ;163273382493569 ; 94017975321952 19415033 ;Camera_1 ; 1 ;272583343210976 ; 94017978831696 19415035 ;Camera_2 ; 1 ;855758511211505 ; 94017973880432 21013374 ;Camera_0 ; 1 ;770316975211505 ; 94017966449648 19415034 ;Camera_4 ; 1 ;597449382494016 ; 94017981830272 19415037 ;Camera_3 ; 2 ;163273396783772 ; 94017979597136 21013374 ;Camera_0 ; 2 ;770316989501708 ; 94017966467888 19415033 ;Camera_1 ; 2 ;272583357501546 ; 94017995748160 19415035 ;Camera_2 ; 2 ;855758525501505 ; 94017978237504 19415034 ;Camera_4 ; 2 ;597449396784219 ; 94017990808464 ... triggerdata.csv flag_0 ;flag_1 ;flag_2 ;flag_3 ;flag_4 ;flag_5 ;flag_6 ;flag_7 ;pulse_id ;uptime_us true ;true ;true ;true ;true ;true ;true ;true ; 0 ;2508068633 false ;true ;true ;true ;true ;true ;true ;true ; 57822 ;3086494592 true ;true ;true ;true ;true ;true ;true ;true ; 155876 ;4067383774 false ;true ;true ;true ;true ;true ;true ;true ; 155876 ;4067383785 true ;true ;true ;true ;true ;true ;true ;true ; 156850 ;4077121115 false ;true ;true ;true ;true ;true ;true ;true ; 163316 ;4141801073 true ;true ;true ;true ;true ;true ;true ;true ; 163316 ;4141801084 false ;true ;true ;true ;true ;true ;true ;true ; 167740 ;4186060281 true ;true ;true ;true ;true ;true ;true ;true ; 171554 ;4224218818 false ;true ;true ;true ;true ;true ;true ;true ; 172716 ;4235837376 ... If your setup is working correctly, pulse_id and frame_id should also align within their time intervals. Please note that if you rely on the external trigger or the in-camera clocks, you will need to correct them for longer recordings as they will drift away from each other, since they are not real-time clocks. Also, unfortunately, the trigger uptime is currently only a 32-bit unsigned integer, which means it overflows every 71 minutes and 35 seconds.","title":"6. How to synchronize your devices"},{"location":"manual/6_synchronization/#how-to-synchronize-your-devices","text":"If you use our external trigger, you can use it's inputs, to synchronize your devices. Most of the supported microcontroller boards log the inputs at pins 16-23 into a /triggerdata.csv file. For time synchronization, the microcontroller boards logs the first inputs at the time the first pulse was sent. This way you can synchronize the first images of the cameras and their internal clock to the internal clock of the external trigger. \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500 \u2500\u2510 ... \u2502 \u2502 \u2502 \u2502 \u2502 ... \u2502 ... \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 wavestate 0 \u2502 1 \u2502 0 \u2502 1 \u2502 0 \u2502 1 n \u2502 0 pulse_count UINT32_MAX \u2502 0 \u2502 0 \u2502 1 \u2502 1 \u2502 2 n \u2502 UINT32_MAX pulse_count (cont.) n \u2502 n+1 \u2502 n+1 \u2502 n+2 \u2502 n+2 \u2502n+3 n+m\u2502 n+m sync_rising_edge == true \u2534 \u2502 sync_rising_edge == false \u2534 The file contents will look similar to this: metadata.csv frame_camera_serial ;frame_camera_name ;frame_id ;frame_timestamp ;frame_image_uid 19415034 ;Camera_4 ; 0 ;597449368203896 ; 94017979513872 19415037 ;Camera_3 ; 0 ;163273368203367 ; 94017967892976 19415033 ;Camera_1 ; 0 ;272583328921019 ; 94017972152896 21013374 ;Camera_0 ; 0 ;770316960921060 ; 94017962792256 19415035 ;Camera_2 ; 0 ;855758496921019 ; 94017968796096 19415037 ;Camera_3 ; 1 ;163273382493569 ; 94017975321952 19415033 ;Camera_1 ; 1 ;272583343210976 ; 94017978831696 19415035 ;Camera_2 ; 1 ;855758511211505 ; 94017973880432 21013374 ;Camera_0 ; 1 ;770316975211505 ; 94017966449648 19415034 ;Camera_4 ; 1 ;597449382494016 ; 94017981830272 19415037 ;Camera_3 ; 2 ;163273396783772 ; 94017979597136 21013374 ;Camera_0 ; 2 ;770316989501708 ; 94017966467888 19415033 ;Camera_1 ; 2 ;272583357501546 ; 94017995748160 19415035 ;Camera_2 ; 2 ;855758525501505 ; 94017978237504 19415034 ;Camera_4 ; 2 ;597449396784219 ; 94017990808464 ... triggerdata.csv flag_0 ;flag_1 ;flag_2 ;flag_3 ;flag_4 ;flag_5 ;flag_6 ;flag_7 ;pulse_id ;uptime_us true ;true ;true ;true ;true ;true ;true ;true ; 0 ;2508068633 false ;true ;true ;true ;true ;true ;true ;true ; 57822 ;3086494592 true ;true ;true ;true ;true ;true ;true ;true ; 155876 ;4067383774 false ;true ;true ;true ;true ;true ;true ;true ; 155876 ;4067383785 true ;true ;true ;true ;true ;true ;true ;true ; 156850 ;4077121115 false ;true ;true ;true ;true ;true ;true ;true ; 163316 ;4141801073 true ;true ;true ;true ;true ;true ;true ;true ; 163316 ;4141801084 false ;true ;true ;true ;true ;true ;true ;true ; 167740 ;4186060281 true ;true ;true ;true ;true ;true ;true ;true ; 171554 ;4224218818 false ;true ;true ;true ;true ;true ;true ;true ; 172716 ;4235837376 ... If your setup is working correctly, pulse_id and frame_id should also align within their time intervals. Please note that if you rely on the external trigger or the in-camera clocks, you will need to correct them for longer recordings as they will drift away from each other, since they are not real-time clocks. Also, unfortunately, the trigger uptime is currently only a 32-bit unsigned integer, which means it overflows every 71 minutes and 35 seconds.","title":"How to synchronize your devices"},{"location":"model_database/model_database/","text":"Pretrained Model Database Welcome to our Model Database! Here you will find all of the pretrained HybridNet Models currently available for download.\\ All the models here are officially supported and we only list models that achieve a high level of accuracy. Please note that these models should be used for pretraining only and you still need to train the network on a dataset created for your specific setup. To include them in your JARVIS installation simply extract them into the pretrained folder inside your main JARVIS-HybridNet directory. Along with each set of models we provide the trainingset that the model was trained with as well as set of recordings for you to validate the models performance with. Monkey Hand Annotated Frames 3000 Number of subjects 1 Dataset Size 0.7 GB Recordings Size 1.3 GB Download Models Download Trainingset Download Recording Human Hand Annotated Frames 3000 Number of subjects 4 Dataset Size 1.5 GB Recordings Size Download Models Coming Soon Coming Soon Rat Full Body Annotated Frames 2000 Number of subjects 1 Dataset Size 1.5 GB Recordings Size 1.3 GB Download Models Coming Soon Coming Soon Mouse Full Body Annotated Frames 3000 Number of subjects 3 Dataset Size 1.5 GB Recordings Size 1.3 GB Download Models Coming Soon Coming Soon","title":"Model Database"},{"location":"model_database/model_database/#pretrained-model-database","text":"Welcome to our Model Database! Here you will find all of the pretrained HybridNet Models currently available for download.\\ All the models here are officially supported and we only list models that achieve a high level of accuracy. Please note that these models should be used for pretraining only and you still need to train the network on a dataset created for your specific setup. To include them in your JARVIS installation simply extract them into the pretrained folder inside your main JARVIS-HybridNet directory. Along with each set of models we provide the trainingset that the model was trained with as well as set of recordings for you to validate the models performance with. Monkey Hand Annotated Frames 3000 Number of subjects 1 Dataset Size 0.7 GB Recordings Size 1.3 GB Download Models Download Trainingset Download Recording Human Hand Annotated Frames 3000 Number of subjects 4 Dataset Size 1.5 GB Recordings Size Download Models Coming Soon Coming Soon Rat Full Body Annotated Frames 2000 Number of subjects 1 Dataset Size 1.5 GB Recordings Size 1.3 GB Download Models Coming Soon Coming Soon Mouse Full Body Annotated Frames 3000 Number of subjects 3 Dataset Size 1.5 GB Recordings Size 1.3 GB Download Models Coming Soon Coming Soon","title":"Pretrained Model Database"}]}